{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/ThomasLamsonFr/AITextGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoModelWithLMHead, \n",
    "    AutoModelForCausalLM, \n",
    "    AutoConfig, \n",
    "    AutoTokenizer,\n",
    "    GPT2Tokenizer, \n",
    "    AdamW,\n",
    "    get_linear_schedule_with_warmup\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_name_or_path = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('cuda:0')\n",
    "print('Device:',device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos el modelo pre-entrenado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/modeling_auto.py:837: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelWithLMHead.from_pretrained(pretrained_name_or_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos del Modelo:\n",
      "=================\n",
      " - Tipo modelo Base: gpt2\n",
      " - Arquitectura: ['GPT2LMHeadModel']\n",
      " - Posiciones (largo m√°ximo de secuencia): 1024\n",
      " - Tama√±o dimensional interno: 768\n",
      " - Cabezales de Atenci√≥n: 12\n",
      " - Capas : 12\n",
      " - Tama√±o de Vocabulario : 50257\n",
      " - Function de Activaci√≥n : gelu_new\n"
     ]
    }
   ],
   "source": [
    "print('Datos del Modelo:\\n=================')\n",
    "print(' - Tipo modelo Base:', model.config.model_type)\n",
    "print(' - Arquitectura:', model.config.architectures)\n",
    "print(' - Posiciones (largo m√°ximo de secuencia):', model.config.n_positions)\n",
    "print(' - Tama√±o dimensional interno:', model.config.n_embd)\n",
    "print(' - Cabezales de Atenci√≥n:', model.config.n_head)\n",
    "print(' - Capas :', model.config.n_layer)\n",
    "print(' - Tama√±o de Vocabulario :', model.config.vocab_size)\n",
    "print(' - Function de Activaci√≥n :', model.config.activation_function)\n",
    "#GELU: https://medium.com/@shoray.goel/gelu-gaussian-error-linear-unit-4ec59fb2e47c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cargamos nuestro tokenizador espa√±ol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_name_or_path);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n",
      "Using mask_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos del Tokenizador:\n",
      "======================\n",
      " - Tama√±o del vocabulario: 50257\n",
      " - Token Inicio de Secuencia (token => id): <|endoftext|> => 50256\n",
      " - Token Fin de Secuencia (token => id): <|endoftext|> => 50256\n",
      " - Token de relleno (token => id): None => None\n",
      " - Token fuera de vocabulario (token => id): <|endoftext|> => <|endoftext|>\n",
      " - Token mascara (token => id): None => None\n",
      " - Largo m√°ximo: 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:1423: FutureWarning: The `max_len` attribute has been deprecated and will be removed in a future version, use `model_max_length` instead.\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "print('Datos del Tokenizador:\\n======================')\n",
    "print(' - Tama√±o del vocabulario:', tokenizer.vocab_size)\n",
    "print(' - Token Inicio de Secuencia (token => id): {} => {}'.format(tokenizer.bos_token, tokenizer.bos_token_id))\n",
    "print(' - Token Fin de Secuencia (token => id): {} => {}'.format(tokenizer.eos_token, tokenizer.eos_token_id))\n",
    "print(' - Token de relleno (token => id): {} => {}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
    "print(' - Token fuera de vocabulario (token => id): {} => {}'.format(tokenizer.unk_token, tokenizer.unk_token))\n",
    "print(' - Token mascara (token => id): {} => {}'.format(tokenizer.mask_token, tokenizer.mask_token))\n",
    "print(' - Largo m√°ximo: {}'.format(tokenizer.max_len))\n",
    "# https://huggingface.co/transformers/main_classes/tokenizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Un ejemplo del tokenizador en funcionamiento\n",
    "Ubica las palabras en su representaci√≥n vectorial ya aprendida, una palabra nueva la descompone utilizando tokens ya conocidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secuencia: Buenos d√≠as a todos\n",
      " - Bu -> 38374\n",
      " - enos -> 28380\n",
      " - d -> 288\n",
      " - √≠ -> 8836\n",
      " - as -> 292\n",
      " - a -> 257\n",
      " - to -> 284\n",
      " - dos -> 37427\n",
      "\n",
      "\n",
      "secuencia: yu8ausy\n",
      " - yu -> 24767\n",
      " - 8 -> 23\n",
      " - aus -> 8717\n",
      " - y -> 88\n",
      "\n",
      "\n",
      "secuencia: 123456\n",
      " - 123 -> 10163\n",
      " - 456 -> 29228\n",
      "\n",
      "\n",
      "secuencia: 3.141592\n",
      " - 3 -> 18\n",
      " - . -> 13\n",
      " - 14 -> 1415\n",
      " - 15 -> 1314\n",
      " - 92 -> 5892\n",
      "\n",
      "\n",
      "secuencia: œÄ\n",
      " - œÄ -> 46582\n",
      "\n",
      "\n",
      "secuencia: ü§î\n",
      " - ÔøΩ -> 8582\n",
      " - ÔøΩ -> 97\n",
      " - ÔøΩ -> 242\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'ü§î'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def show_tokenizer_working(seq, tokenizer):\n",
    "    toks = tokenizer.encode(seq)\n",
    "    print (\"secuencia:\", seq)\n",
    "    for tok in toks:\n",
    "        print ( \" - {} -> {}\".format(tokenizer.decode([tok]).strip(),tok))\n",
    "    print('\\n')\n",
    "show_tokenizer_working('Buenos d√≠as a todos',tokenizer)\n",
    "show_tokenizer_working('yu8ausy',tokenizer)\n",
    "show_tokenizer_working('123456',tokenizer)\n",
    "show_tokenizer_working('3.141592',tokenizer)\n",
    "show_tokenizer_working('œÄ',tokenizer)\n",
    "show_tokenizer_working('ü§î',tokenizer)\n",
    "#aunque el tokenizador no conoce el emoji, es capaz de codificarlo y decodificarlo\n",
    "tokenizer.decode(tokenizer.encode('ü§î'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vamos a agregar al tokenizador nuestros tokens especiales nuevos\n",
    "\n",
    "Cada tweet lo vamos a estructurar de la siguiente forma para entregarle a nuestro modelo cada ejemplo as√≠:\n",
    "\n",
    "|Coalici√≥n|Partido|Sentimiento|Entidades|Frases Clave| tweet |\n",
    "|---      |---    |---        |---      |---           | ---| \n",
    "|[COALICION] chile vamos|[PARTIDO] udi |[SENTIMIENTO] positivo |[ENTIDADES] carabineros  |[FRASES] cuentan con el apoyo| [TWEET] ahora los carabineros se enfrentan..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.add_special_tokens(\n",
    "    {'bos_token': '[TWEET]',\n",
    "     'additional_special_tokens': ['[COALICION]', '[PARTIDO]', '[SENTIMIENTO]', '[ENTIDADES]', '[HASHTAGS]', '[FRASES]']})\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(50264, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datos del Modelo:\n",
      "=================\n",
      " - Tipo modelo Base: gpt2\n",
      " - Arquitectura: ['GPT2LMHeadModel']\n",
      " - Posiciones (largo m√°ximo de secuencia): 1024\n",
      " - Tama√±o dimensional interno: 768\n",
      " - Cabezales de Atenci√≥n: 12\n",
      " - Capas : 12\n",
      " - Tama√±o de Vocabulario : 50264\n",
      " - Function de Activaci√≥n : gelu_new\n"
     ]
    }
   ],
   "source": [
    "print('Datos del Modelo:\\n=================')\n",
    "print(' - Tipo modelo Base:', model.config.model_type)\n",
    "print(' - Arquitectura:', model.config.architectures)\n",
    "print(' - Posiciones (largo m√°ximo de secuencia):', model.config.n_positions)\n",
    "print(' - Tama√±o dimensional interno:', model.config.n_embd)\n",
    "print(' - Cabezales de Atenci√≥n:', model.config.n_head)\n",
    "print(' - Capas :', model.config.n_layer)\n",
    "print(' - Tama√±o de Vocabulario :', model.config.vocab_size)\n",
    "print(' - Function de Activaci√≥n :', model.config.activation_function)\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>COALICION</th>\n",
       "      <th>PARTIDO</th>\n",
       "      <th>SENTIMIENTO</th>\n",
       "      <th>ENTIDADES</th>\n",
       "      <th>HASHTAGS</th>\n",
       "      <th>FRASES</th>\n",
       "      <th>TWEET</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Chile Vamos</td>\n",
       "      <td>IND-GOB</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>CarolCBown s_villarrealb sebastianpinera</td>\n",
       "      <td>CuentaP√∫blica ChileenMarcha</td>\n",
       "      <td></td>\n",
       "      <td>Ya estamos en el Congreso con los subses @Caro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chile Vamos</td>\n",
       "      <td>RN</td>\n",
       "      <td>NEGATIVE</td>\n",
       "      <td></td>\n",
       "      <td>Araucan√≠a CuentaP√∫blica</td>\n",
       "      <td></td>\n",
       "      <td>‚≠ï \"Combatir con m√°xima voluntad y firmeza, sie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chile Vamos</td>\n",
       "      <td>RN</td>\n",
       "      <td>NEUTRAL</td>\n",
       "      <td>Presidente Ministerio de Agricultura y Aliment...</td>\n",
       "      <td>CuentaP√∫blica</td>\n",
       "      <td>#CuentaP√∫blica ANUNCIO Nuestro Presidente la c...</td>\n",
       "      <td>#CuentaP√∫blica\\nüì¢ANUNCIO| Nuestro Presidente a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     COALICION  PARTIDO SENTIMIENTO  \\\n",
       "0  Chile Vamos  IND-GOB     NEUTRAL   \n",
       "1  Chile Vamos       RN    NEGATIVE   \n",
       "2  Chile Vamos       RN     NEUTRAL   \n",
       "\n",
       "                                           ENTIDADES  \\\n",
       "0           CarolCBown s_villarrealb sebastianpinera   \n",
       "1                                                      \n",
       "2  Presidente Ministerio de Agricultura y Aliment...   \n",
       "\n",
       "                      HASHTAGS  \\\n",
       "0  CuentaP√∫blica ChileenMarcha   \n",
       "1      Araucan√≠a CuentaP√∫blica   \n",
       "2                CuentaP√∫blica   \n",
       "\n",
       "                                              FRASES  \\\n",
       "0                                                      \n",
       "1                                                      \n",
       "2  #CuentaP√∫blica ANUNCIO Nuestro Presidente la c...   \n",
       "\n",
       "                                               TWEET  \n",
       "0  Ya estamos en el Congreso con los subses @Caro...  \n",
       "1  ‚≠ï \"Combatir con m√°xima voluntad y firmeza, sie...  \n",
       "2  #CuentaP√∫blica\\nüì¢ANUNCIO| Nuestro Presidente a...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('tweets_formatted.json', lines=True)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorboard in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (2.4.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (0.4.2)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (0.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (3.3.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (1.7.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (3.11.4)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (1.0.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (0.34.2)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (1.14.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (1.23.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (2.22.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (1.33.2)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (45.2.0.post20200210)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from tensorboard) (1.18.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard) (2.0.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (4.5)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from google-auth<2,>=1.6.3->tensorboard) (0.2.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (1.25.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests<3,>=2.21.0->tensorboard) (2020.6.20)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard) (3.1.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard) (2.2.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard) (0.4.8)\n",
      "\u001b[33mWARNING: You are using pip version 20.0.2; however, version 20.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_latest_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153275 17031\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from src.torch_loader import  DatasetFromPandas, VectorizeMode, VectorizeParagraph\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\n",
    "\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import random\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "train_df, test_df = train_test_split(df,test_size=0.1)\n",
    "print(len(train_df), len(test_df))\n",
    "\n",
    "\n",
    "GPT2_BLOCK_SIZE = model.config.n_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = VectorizeParagraph(\n",
    "    tokenizer=tokenizer,\n",
    "    block_size=GPT2_BLOCK_SIZE,\n",
    "    mode=VectorizeMode.TRAIN\n",
    ")\n",
    "\n",
    "train_ds = DatasetFromPandas(train_df, vectorizer)\n",
    "eval_ds = DatasetFromPandas(test_df, vectorizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorboard\n",
    "\n",
    "n_gpu = 1\n",
    "\n",
    "train_batch_size = 4\n",
    "per_gpu_train_batch_size = 4\n",
    "\n",
    "eval_batch_size = 8\n",
    "per_gpu_eval_batch_size = 8\n",
    "\n",
    "gradient_accumulation_steps = 1\n",
    "weight_decay = 0.0\n",
    "learning_rate = 5e-5\n",
    "warmup_steps = 0\n",
    "adam_epsilon = 1e-8\n",
    "max_grad_norm = 1\n",
    "logging_steps= 2000\n",
    "save_steps = 2000\n",
    "output_dir = 'model_checkpoints_v3'\n",
    "print_input=False\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "def collate(examples):\n",
    "    all_inputs = [elt[0] for elt in examples]\n",
    "    all_types = [elt[1] for elt in examples]\n",
    "    all_labels = [elt[2] for elt in examples]\n",
    "\n",
    "    padded_inputs = pad_sequence(\n",
    "        all_inputs, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    padded_types = pad_sequence(\n",
    "        all_types, batch_first=True, padding_value=tokenizer.pad_token_id\n",
    "    )\n",
    "    padded_labels = pad_sequence(all_labels, batch_first=True, padding_value=-100)\n",
    "\n",
    "    return padded_inputs, padded_types, padded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataset, model, tokenizer, epochs):\n",
    "    tb_writer = SummaryWriter()\n",
    "    num_train_epochs = epochs\n",
    "    train_sampler = RandomSampler(train_dataset)\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=train_batch_size,\n",
    "        collate_fn=collate,\n",
    "    )\n",
    "    \n",
    "    t_total = len(train_dataloader) // gradient_accumulation_steps * num_train_epochs\n",
    "\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "    \n",
    "    optimizer_grouped_parameters = [\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if not any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": weight_decay,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [\n",
    "                p\n",
    "                for n, p in model.named_parameters()\n",
    "                if any(nd in n for nd in no_decay)\n",
    "            ],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    optimizer = AdamW(\n",
    "        optimizer_grouped_parameters, lr=learning_rate, eps=adam_epsilon\n",
    "    )\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "    \n",
    "    if n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    model.to(device)\n",
    "    #Entrenar!\n",
    "    \n",
    "    print(\"***** Running training *****\")\n",
    "    print(\"  Num examples =\", len(train_dataset))\n",
    "    print(\"  Num Epochs =\", num_train_epochs)\n",
    "    print(\"  Instantaneous batch size per GPU =\", per_gpu_train_batch_size)\n",
    "    print(\"  Gradient Accumulation steps =\", gradient_accumulation_steps)\n",
    "    print(\"  Total optimization steps =\", t_total)\n",
    "    \n",
    "    global_step = 0\n",
    "    epochs_trained = 0\n",
    "    steps_trained_in_current_epoch = 0\n",
    "    \n",
    "    tr_loss, logging_loss = 0.0, 0.0\n",
    "    \n",
    "    model_to_resize = model.module if hasattr(model, \"module\") else model\n",
    "    model_to_resize.resize_token_embeddings(len(tokenizer))   \n",
    "    \n",
    "    model.zero_grad()\n",
    "    \n",
    "    train_iterator = trange(\n",
    "        epochs_trained,\n",
    "        int(num_train_epochs),\n",
    "        desc=\"Epoch\",\n",
    "        disable=False,\n",
    "    )\n",
    "    \n",
    "    set_seed(0)\n",
    "    \n",
    "    for ti in train_iterator:\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=False)\n",
    "        \n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            \n",
    "            input_ids, type_ids, labels = batch\n",
    "\n",
    "            if print_input:\n",
    "                print( \"Examples contained in the batch that will be given as input in the model\")\n",
    "                \n",
    "                for i in range(input_ids.shape[0]):\n",
    "                    decoded_input = tokenizer.decode(input_ids[i, :].tolist(), skip_special_tokens=False)\n",
    "                    print(\"Ex n¬∞ {} : {}\".format(i, decoded_input)) \n",
    "                \n",
    "            input_ids = input_ids.to(device)\n",
    "            type_ids = type_ids.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            model.train()\n",
    "            \n",
    "            outputs = model(input_ids, labels=labels, token_type_ids=type_ids)\n",
    "            loss = outputs[ 0]  # model outputs are always tuple in transformers (see doc)\n",
    "\n",
    "            if n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if gradient_accumulation_steps > 1:\n",
    "                loss = loss / gradient_accumulation_steps\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "            \n",
    "            if (step + 1) % gradient_accumulation_steps == 0:\n",
    "\n",
    "                torch.nn.utils.clip_grad_norm_( model.parameters(), max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()  # Update learning rate schedule\n",
    "                model.zero_grad()\n",
    "                global_step += 1\n",
    "\n",
    "                if ( logging_steps > 0 and global_step % logging_steps == 0):\n",
    "                    results = evaluate(model, tokenizer)\n",
    "                    for key, value in results.items():\n",
    "                        tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "                    tb_writer.add_scalar(\n",
    "                        \"loss\",\n",
    "                        (tr_loss - logging_loss) / logging_steps,\n",
    "                        global_step,\n",
    "                    )\n",
    "                    logging_loss = tr_loss\n",
    "\n",
    "                if (global_step % save_steps == 0):\n",
    "                    checkpoint_prefix = \"checkpoint\"\n",
    "                    # Save model checkpoint\n",
    "                    train_output_dir = os.path.join(\n",
    "                        output_dir, \"{}-{}\".format(checkpoint_prefix, global_step)\n",
    "                    )\n",
    "                    os.makedirs(train_output_dir, exist_ok=True)\n",
    "                    model_to_save = (\n",
    "                        model.module if hasattr(model, \"module\") else model\n",
    "                    )  # Take care of distributed/parallel training\n",
    "                    model_to_save.save_pretrained(train_output_dir)\n",
    "                    tokenizer.save_pretrained(train_output_dir)\n",
    "\n",
    "                    #torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                    print(\"Saving model checkpoint to %s\", train_output_dir)\n",
    "\n",
    "\n",
    "                    torch.save(\n",
    "                        optimizer.state_dict(), os.path.join(train_output_dir, \"optimizer.pt\")\n",
    "                    )\n",
    "                    torch.save(\n",
    "                        scheduler.state_dict(), os.path.join(train_output_dir, \"scheduler.pt\")\n",
    "                    )\n",
    "                    print(\n",
    "                        \"Saving optimizer and scheduler states to %s\", train_output_dir\n",
    "                    )\n",
    "            \n",
    "    tb_writer.close()\n",
    "    return global_step, tr_loss / global_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, prefix=\"\"):\n",
    "    eval_output_dir = output_dir\n",
    "    eval_dataset = eval_ds\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "    eval_batch_size = per_gpu_eval_batch_size * n_gpu\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        sampler=eval_sampler,\n",
    "        batch_size=eval_batch_size,\n",
    "        collate_fn=collate,\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    model.to(device)\n",
    "\n",
    "    # Eval!\n",
    "    print(\"***** Running evaluation {} *****\".format(prefix))\n",
    "    print(\"  Num examples =\", len(eval_dataset))\n",
    "    print(\"  Batch size =\",eval_batch_size)\n",
    "    \n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        inputs, types, labels = batch\n",
    "        dev = torch.device(\"cuda:0\")\n",
    "        \n",
    "        inputs = inputs.to(dev)\n",
    "        types  = types.to(dev)\n",
    "        labels = labels.to(dev)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs, labels=labels, token_type_ids=types)\n",
    "            lm_loss = outputs[0]\n",
    "            eval_loss += lm_loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"w\") as writer:\n",
    "        print(\"***** Eval results {} *****\".format(prefix))\n",
    "        for key in sorted(result.keys()):\n",
    "            text = \"{} = {}\".format(key, str(result[key]))\n",
    "            print(text)\n",
    "            writer.write(text)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 153275\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per GPU = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 114957\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61571bb5c307470cb1c7c5aa2d9b4243",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Epoch', max=3.0, style=ProgressStyle(description_width='i‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58bd1b0dad94459995ad8ddc9ced1797",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=38319.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a857a253e434198a535be432ffcc6ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(7.6105)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:247: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
      "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_latest_p36/lib/python3.6/site-packages/torch/optim/lr_scheduler.py:216: UserWarning: Please also save or load the state of the optimizer when saving or loading the scheduler.\n",
      "  warnings.warn(SAVE_STATE_WARNING, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-2000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85ea2cb1a3d43298ca166b45af80bc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(6.5305)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-4000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-4000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb9711f440744be9f0e052d262065d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(5.9847)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-6000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-6000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0af9c115ea5344e09138977d4d76d454",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(5.6638)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-8000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-8000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a5b77e7263422cbb488c1eb0e2e1df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(5.4246)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-10000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-10000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4f49748f54444a69e687fb1f56dc9f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(5.2211)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-12000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-12000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ddeb5a0caf4488bddee7648e1ed8f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(5.0556)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-14000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-14000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "186387f228a84428830e6db6f6fb70b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.9020)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-16000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-16000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51587cf3f385453f84d430022852497b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.7702)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-18000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-18000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0127ef1b056e4586871646fa297599b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.6802)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-20000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-20000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b593c4a300254ff6903ee22437923275",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.5961)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-22000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-22000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab8c8e521cd74791b03635ccd81f3d8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.5415)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-24000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-24000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92aa47dec70d43c39f46c4f39db6d204",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.4495)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-26000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-26000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a34dbb06554e708623430c0fff6ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.3845)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-28000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-28000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "862b8e2895574e9f8477b0c5939530e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.3374)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-30000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-30000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "399151a17b914087abab2928ba5229de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.2949)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-32000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-32000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e1267c6bf5d45f48387469ac9c40f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.2327)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-34000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-34000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12eaea338ed74ffbaa3a14be187273f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.1934)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-36000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-36000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3edef91110ef46a68968fe85670834bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.1503)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-38000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-38000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "505fa63269a842a98a32a64fd38b5b50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=38319.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f38e046cec24a44aad50783d211252b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.1304)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-40000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-40000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "767ea4b76a654f9ab5fa66579bc0106e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.0832)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-42000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-42000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06f6dde6f1743c4896fbb3c4280fea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.0448)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-44000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-44000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bd40f860d6941f0940c4addcaff5f5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.0179)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-46000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-46000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3cdb12d84f5e4284b7a821602adfe29e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(4.0137)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-48000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-48000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3b6e52fcf4b400b96f0a3e6a9dfe553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.9629)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-50000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-50000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f4365d9d9f4acb8e2e50dcd7c7dd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.9423)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-52000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-52000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaaa313cb7a2420db717b9aea04c95f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.9245)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-54000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-54000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c493556392f4e26bc020c84ca6f1b92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.9004)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-56000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-56000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07bd05976c449a1a2305ce1d57a3ca3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.8886)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-58000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-58000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca1a0d1e5ec9489ab25eb122933e58cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.8711)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-60000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-60000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf534ab7695473982cad600e4a83bdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.8465)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-62000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-62000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80ba64d16cf8435ea9377f413fd28ed9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.8235)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-64000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-64000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e985c0c87b9a419f9c96adc142148c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.8114)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-66000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-66000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92f58d33d00f4840b45e83c7c4d4a440",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.7948)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-68000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-68000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c2b4840e9a4402eab58ac35c17afda5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.7725)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-70000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-70000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8639d16a446f4534a37116c4ac454e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.7496)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-72000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-72000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3342da3c93b6483592119759ad903939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.7477)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-74000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-74000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95fe32eb234e4bcc99b33196edbe5de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.7243)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-76000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-76000\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c042cc2299804231b19c87cd00354299",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Iteration', max=38319.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dff68815148491bba79bf441704e530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.7323)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-78000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-78000\n",
      "***** Running evaluation  *****\n",
      "  Num examples = 17031\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e82acea0c0864ffba908954cef118c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Evaluating', max=2129.0, style=ProgressStyle(description_‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Eval results  *****\n",
      "perplexity = tensor(3.7182)\n",
      "Saving model checkpoint to %s model_checkpoints_v3/checkpoint-80000\n",
      "Saving optimizer and scheduler states to %s model_checkpoints_v3/checkpoint-80000\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "global_step, tr_loss = train(train_ds, model, tokenizer, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fail\n"
     ]
    }
   ],
   "source": [
    "print('fail')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_latest_p36",
   "language": "python",
   "name": "conda_pytorch_latest_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
