{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip uninstall tensorflow -y\n",
    "#!pip uninstall transformers -y\n",
    "#!pip install tensorflow==2.3.1\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import transformers\n",
    "from transformers import GPT2Config\n",
    "from transformers import TFGPT2LMHeadModel\n",
    "from transformers import GPT2Tokenizer\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow_model_optimization\n",
    "#import tensorflow_model_optimization as tfmot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.optimizer.set_jit(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_final_path = './dataset_final'\n",
    "tokenized_data_path = './tokenized_data'\n",
    "checkpoint_dir = \"./ckpt3\"\n",
    "\n",
    "# Prepare a directory to store all the checkpoints.\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset:\n",
      " 10872699 (TensorSpec(shape=(99,), dtype=tf.int32, name=None), TensorSpec(shape=(99,), dtype=tf.int32, name=None))\n",
      "Shuffled Dataset:\n",
      " 169885 <BatchDataset shapes: ((64, 99), (64, 99)), types: (tf.int32, tf.int32)>\n",
      "Tokenizer Vocab Size: 50000\n"
     ]
    }
   ],
   "source": [
    "element_specs = (tf.TensorSpec(shape=(99,), dtype=tf.int32, name=None),\n",
    " tf.TensorSpec(shape=(99,), dtype=tf.int32, name=None))\n",
    "#Solo seleccionamos 10000 para proba\n",
    "dataset = tf.data.experimental.load(dataset_final_path,element_specs)#.take(10000)\n",
    "print(\"Dataset:\\n\",len(dataset), dataset.element_spec)\n",
    "\n",
    "\n",
    "BATCH_SIZE = 8*4*2\n",
    "BUFFER_SIZE = 10000\n",
    "shuffled_dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "print(\"Shuffled Dataset:\\n\",len(shuffled_dataset), shuffled_dataset)\n",
    "shuffled_dataset = shuffled_dataset.cache()\n",
    "shuffled_dataset = shuffled_dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(tokenized_data_path)\n",
    "tokenizer.add_special_tokens({\n",
    "  \"eos_token\": \"</s>\",\n",
    "  \"bos_token\": \"<s>\",\n",
    "  \"unk_token\": \"<unk>\",\n",
    "  \"pad_token\": \"<pad>\",\n",
    "  \"mask_token\": \"<mask>\"\n",
    "})# creating the configurations from which the model can be made\n",
    "print('Tokenizer Vocab Size:', tokenizer.vocab_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compiled_model():\n",
    "    print(\"Compilando modelo\\n=============\")\n",
    "    config = GPT2Config(\n",
    "        vocab_size = tokenizer.vocab_size,\n",
    "        n_positions = 512,\n",
    "        n_ctx = 512, \n",
    "        bos_token_id=tokenizer.bos_token_id,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        pad_token_id = tokenizer.pad_token_id,\n",
    "        n_head=16 #n_head==Batch Size / GPUs\n",
    "    )# creating the model\n",
    "    \n",
    "    model = TFGPT2LMHeadModel(config)\n",
    "    # defining our optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, \n",
    "                                         #clipnorm=1.0 \n",
    "                                        )\n",
    "    # definining our loss function\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "    # defining our metric which we want to observe\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "    # compiling the model\n",
    "    model.compile(optimizer=optimizer, loss=[loss, *[None] * model.config.n_layer], metrics=[metric])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf logs_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_or_restore_model():\n",
    "    latest = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "    print('Último Checkpoint:', latest)\n",
    "    model = get_compiled_model()\n",
    "    \n",
    "    if latest:\n",
    "        print('Restaurando desde :', latest)\n",
    "        model.load_weights(latest)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "def run_training(epochs=1):\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "    callbacks = [\n",
    "    # This callback saves a SavedModel every epoch\n",
    "    # We include the current epoch in the folder name.\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        verbose=1,\n",
    "        filepath=checkpoint_dir + \"/ckpt-{epoch}\", save_freq=\"epoch\",\n",
    "        save_weights_only=True\n",
    "    ),\n",
    "    tf.keras.callbacks.TensorBoard(log_dir=\"./logs_3\", write_graph=True, update_freq=1000, profile_batch=1000, embeddings_freq=1000)]\n",
    "    # Open a strategy scope and create/restore the model\n",
    "    with strategy.scope():\n",
    "        model = make_or_restore_model()\n",
    "        \n",
    "    model.fit(shuffled_dataset, epochs=epochs, callbacks=callbacks, verbose=1)\n",
    "        \n",
    "    return model\n",
    "\n",
    "    \n",
    "    \n",
    "bad_words_ids =[ \n",
    "    tokenizer.encode('==', add_prefix_space=True), \n",
    "    tokenizer.encode('===', add_prefix_space=True),  \n",
    "    tokenizer.encode('====', add_prefix_space=True),\n",
    "    tokenizer.encode('<pad>', add_prefix_space=True)]\n",
    "\n",
    "\n",
    "def test_model(model,text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='tf')# getting out output\n",
    "    print(\"generando beams para input: \", text,\"\\n====================================\")\n",
    "    beam_output = model.generate(\n",
    "      input_ids,\n",
    "      max_length = 100,\n",
    "      num_beams = 20,\n",
    "      bad_words_ids = bad_words_ids,\n",
    "      temperature = 1,\n",
    "      #length_penalty=1,\n",
    "      no_repeat_ngram_size=2,\n",
    "      repetition_penalty=2,\n",
    "      num_return_sequences=5\n",
    "    )\n",
    "    \n",
    "    decoded_beams = []\n",
    "    \n",
    "    for beam in beam_output:   \n",
    "        decoded_beams.append(tokenizer.decode(beam))\n",
    "        print (\"\\n\\n\",tokenizer.decode(beam))\n",
    "        \n",
    "    return decoded_beams\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Epoch 1-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3')\n",
      "Último Checkpoint: None\n",
      "Compilando modelo\n",
      "=============\n",
      "Epoch 1/5\n",
      "INFO:tensorflow:batch_all_reduce: 147 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 147 all-reduces with algorithm = nccl, num_packs = 1\n",
      "WARNING:tensorflow:Efficient allreduce is not supported for 1 IndexedSlices\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:GPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1', '/job:localhost/replica:0/task:0/device:GPU:2', '/job:localhost/replica:0/task:0/device:GPU:3').\n",
      "   999/169885 [..............................] - ETA: 8:58:18 - loss: 7.4331 - output_1_loss: 7.4331 - output_1_accuracy: 0.1097 - output_2_1_accuracy: 0.0020 - output_2_2_accuracy: 0.0021 - output_2_3_accuracy: 0.0023 - output_2_4_accuracy: 0.0023 - output_2_5_accuracy: 0.0023 - output_2_6_accuracy: 0.0019 - output_2_7_accuracy: 0.0016 - output_2_8_accuracy: 0.0020 - output_2_9_accuracy: 0.0014 - output_2_10_accuracy: 0.0016 - output_2_11_accuracy: 0.0019 - output_2_12_accuracy: 0.0027WARNING:tensorflow:From /home/ec2-user/anaconda3/envs/tensorflow2_p36/lib/python3.6/site-packages/tensorflow/python/ops/summary_ops_v2.py:1277: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
      "Instructions for updating:\n",
      "use `tf.profiler.experimental.stop` instead.\n",
      "169885/169885 [==============================] - ETA: 0s - loss: 3.3833 - output_1_loss: 3.3833 - output_1_accuracy: 0.3840 - output_2_1_accuracy: 0.0022 - output_2_2_accuracy: 0.0026 - output_2_3_accuracy: 0.0018 - output_2_4_accuracy: 0.0017 - output_2_5_accuracy: 0.0019 - output_2_6_accuracy: 0.0020 - output_2_7_accuracy: 0.0022 - output_2_8_accuracy: 0.0023 - output_2_9_accuracy: 0.0020 - output_2_10_accuracy: 0.0019 - output_2_11_accuracy: 0.0022 - output_2_12_accuracy: 0.0020\n",
      "Epoch 00003: saving model to ./ckpt3/ckpt-3\n",
      "169885/169885 [==============================] - 32764s 193ms/step - loss: 3.3833 - output_1_loss: 3.3833 - output_1_accuracy: 0.3840 - output_2_1_accuracy: 0.0022 - output_2_2_accuracy: 0.0026 - output_2_3_accuracy: 0.0018 - output_2_4_accuracy: 0.0017 - output_2_5_accuracy: 0.0019 - output_2_6_accuracy: 0.0020 - output_2_7_accuracy: 0.0022 - output_2_8_accuracy: 0.0023 - output_2_9_accuracy: 0.0020 - output_2_10_accuracy: 0.0019 - output_2_11_accuracy: 0.0022 - output_2_12_accuracy: 0.0020\n",
      "Epoch 4/5\n",
      "169885/169885 [==============================] - ETA: 0s - loss: 3.2834 - output_1_loss: 3.2834 - output_1_accuracy: 0.3953 - output_2_1_accuracy: 0.0022 - output_2_2_accuracy: 0.0027 - output_2_3_accuracy: 0.0018 - output_2_4_accuracy: 0.0017 - output_2_5_accuracy: 0.0019 - output_2_6_accuracy: 0.0020 - output_2_7_accuracy: 0.0022 - output_2_8_accuracy: 0.0022 - output_2_9_accuracy: 0.0021 - output_2_10_accuracy: 0.0019 - output_2_11_accuracy: 0.0022 - output_2_12_accuracy: 0.0020\n",
      "Epoch 00005: saving model to ./ckpt3/ckpt-5\n",
      "169885/169885 [==============================] - 32764s 193ms/step - loss: 3.2834 - output_1_loss: 3.2834 - output_1_accuracy: 0.3953 - output_2_1_accuracy: 0.0022 - output_2_2_accuracy: 0.0027 - output_2_3_accuracy: 0.0018 - output_2_4_accuracy: 0.0017 - output_2_5_accuracy: 0.0019 - output_2_6_accuracy: 0.0020 - output_2_7_accuracy: 0.0022 - output_2_8_accuracy: 0.0022 - output_2_9_accuracy: 0.0021 - output_2_10_accuracy: 0.0019 - output_2_11_accuracy: 0.0022 - output_2_12_accuracy: 0.0020\n"
     ]
    }
   ],
   "source": [
    "history = run_training(epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "123.849216"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.count_params()/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2Config {\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"attn_pdrop\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"embd_pdrop\": 0.1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"model_type\": \"gpt2\",\n",
       "  \"n_ctx\": 512,\n",
       "  \"n_embd\": 768,\n",
       "  \"n_head\": 16,\n",
       "  \"n_inner\": null,\n",
       "  \"n_layer\": 12,\n",
       "  \"n_positions\": 512,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"resid_pdrop\": 0.1,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"vocab_size\": 50000\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_words_ids =[ \n",
    "    tokenizer.encode('==', add_prefix_space=True), \n",
    "    tokenizer.encode('===', add_prefix_space=True),  \n",
    "    tokenizer.encode('====', add_prefix_space=True),\n",
    "    tokenizer.encode('<pad>', add_prefix_space=True)]\n",
    "\n",
    "\n",
    "def test_model(model,text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='tf')# getting out output\n",
    "    print(\"generando beams para input: \", text,\"\\n====================================\")\n",
    "    beam_output = model.generate(\n",
    "      input_ids,\n",
    "      max_length = 100,\n",
    "      num_beams = 20,\n",
    "      bad_words_ids = bad_words_ids,\n",
    "      temperature = 1,\n",
    "      #length_penalty=1,\n",
    "      no_repeat_ngram_size=2,\n",
    "      repetition_penalty=2,\n",
    "      num_return_sequences=5\n",
    "    )\n",
    "    \n",
    "    decoded_beams = []\n",
    "    \n",
    "    for beam in beam_output:   \n",
    "        decoded_beams.append(tokenizer.decode(beam))\n",
    "        print (\"\\n\\n\",tokenizer.decode(beam))\n",
    "        \n",
    "    return decoded_beams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generando beams para input:  El 27 de enero del año 2010 \n",
      "====================================\n",
      "\n",
      "\n",
      " El 27 de enero del año 2010, se anunció que la serie había sido renovada para una segunda temporada. La primera parte fue estrenada el 13 julio 2011 en Estados Unidos por ABC Family como un spin-off titulado ''NCIS: Naval Criminal Investigative Service''. El 16 agosto 2012 salió al aire su tercera última película titulada \"The Big Bang Theory\", basada libremente sobre los acontecimientos ocurridos durante las Guerras Clon antes después de Crisis on Infinite Earths (Crisis Final)). En esta nueva versión\n",
      "\n",
      "\n",
      " El 27 de enero del año 2010, se anunció que la serie había sido renovada para una segunda temporada. La primera parte fue estrenada el 13 julio 2011 en Estados Unidos por ABC Family como un spin-off titulado ''NCIS: Naval Criminal Investigative Service''. El 11 septiembre 2012 salió al aire su tercera última película titulada \"The Big Bang Theory\", basada libremente sobre los acontecimientos ocurridos durante las Guerras Clon antes después de Crisis on Infinite Earths (Crisis Final)). En esta nueva versión\n",
      "\n",
      "\n",
      " El 27 de enero del año 2010, se anunció que la serie había sido renovada para una segunda temporada. La primera parte fue estrenada el 13 julio 2011 en Estados Unidos por ABC Family como un spin-off titulado ''NCIS: Naval Criminal Investigative Service''. El 11 septiembre 2012 salió al aire su tercera última película titulada \"The Big Bang Theory\", basada libremente sobre los acontecimientos ocurridos durante las Guerras Clon antes después de Crisis on Infinite Earths (Crisis Final)). En esta nueva producción\n",
      "\n",
      "\n",
      " El 27 de enero del año 2010, se anunció que la serie había sido renovada para una segunda temporada. La primera parte fue estrenada el 13 julio 2011 en Estados Unidos por ABC Family como un spin-off titulado ''NCIS: Naval Criminal Investigative Service''. El 16 agosto 2012 salió al aire su tercera última película titulada \"The Big Bang Theory\", basada libremente sobre los acontecimientos ocurridos durante las Guerras Clon antes después de Crisis on Infinite Earths (Crisis Final)). En esta nueva producción\n",
      "\n",
      "\n",
      " El 27 de enero del año 2010, se anunció que la serie había sido renovada para una segunda temporada. La primera parte fue estrenada el 13 julio 2011 en Estados Unidos por ABC Family como un spin-off titulado ''NCIS: Naval Criminal Investigative Service''. El 11 septiembre 2012 salió al aire su tercera última película titulada \"The Big Bang Theory\", basada libremente sobre los acontecimientos ocurridos durante las Guerras Clon antes después de Crisis on Infinite Earths (Crisis Final)). En esta nueva etapa\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['El 27 de enero del año 2010, se anunció que la serie había sido renovada para una segunda temporada. La primera parte fue estrenada el 13 julio 2011 en Estados Unidos por ABC Family como un spin-off titulado \\'\\'NCIS: Naval Criminal Investigative Service\\'\\'. El 16 agosto 2012 salió al aire su tercera última película titulada \"The Big Bang Theory\", basada libremente sobre los acontecimientos ocurridos durante las Guerras Clon antes después de Crisis on Infinite Earths (Crisis Final)). En esta nueva versión',\n",
       " 'El 27 de enero del año 2010, se anunció que la serie había sido renovada para una segunda temporada. La primera parte fue estrenada el 13 julio 2011 en Estados Unidos por ABC Family como un spin-off titulado \\'\\'NCIS: Naval Criminal Investigative Service\\'\\'. El 11 septiembre 2012 salió al aire su tercera última película titulada \"The Big Bang Theory\", basada libremente sobre los acontecimientos ocurridos durante las Guerras Clon antes después de Crisis on Infinite Earths (Crisis Final)). En esta nueva versión',\n",
       " 'El 27 de enero del año 2010, se anunció que la serie había sido renovada para una segunda temporada. La primera parte fue estrenada el 13 julio 2011 en Estados Unidos por ABC Family como un spin-off titulado \\'\\'NCIS: Naval Criminal Investigative Service\\'\\'. El 11 septiembre 2012 salió al aire su tercera última película titulada \"The Big Bang Theory\", basada libremente sobre los acontecimientos ocurridos durante las Guerras Clon antes después de Crisis on Infinite Earths (Crisis Final)). En esta nueva producción',\n",
       " 'El 27 de enero del año 2010, se anunció que la serie había sido renovada para una segunda temporada. La primera parte fue estrenada el 13 julio 2011 en Estados Unidos por ABC Family como un spin-off titulado \\'\\'NCIS: Naval Criminal Investigative Service\\'\\'. El 16 agosto 2012 salió al aire su tercera última película titulada \"The Big Bang Theory\", basada libremente sobre los acontecimientos ocurridos durante las Guerras Clon antes después de Crisis on Infinite Earths (Crisis Final)). En esta nueva producción',\n",
       " 'El 27 de enero del año 2010, se anunció que la serie había sido renovada para una segunda temporada. La primera parte fue estrenada el 13 julio 2011 en Estados Unidos por ABC Family como un spin-off titulado \\'\\'NCIS: Naval Criminal Investigative Service\\'\\'. El 11 septiembre 2012 salió al aire su tercera última película titulada \"The Big Bang Theory\", basada libremente sobre los acontecimientos ocurridos durante las Guerras Clon antes después de Crisis on Infinite Earths (Crisis Final)). En esta nueva etapa']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model(history, \"El 27 de enero del año 2010\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardando el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WEIGHTS_NAME, CONFIG_NAME\n",
    "\n",
    "import os\n",
    "\n",
    "output_dir = './model_es_8heads_256pos/'\n",
    "\n",
    "\n",
    "# creating directory if it is not present\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "model_to_save = history.module if hasattr(history, 'module') else history\n",
    "\n",
    "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
    "output_config_file = os.path.join(output_dir, CONFIG_NAME)\n",
    "\n",
    "\n",
    "# save model and model configs\n",
    "history.save_pretrained(output_dir)\n",
    "model_to_save.config.to_json_file(output_config_file)\n",
    "\n",
    "# save tokenizer\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow2_p36",
   "language": "python",
   "name": "conda_tensorflow2_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
